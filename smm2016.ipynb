{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "4bffcdd8-c3a5-456e-8289-23678137d058"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, Image, Markdown\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(\"microTC\")\n",
    "from microtc.textmodel import TextModel\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "from graphviz import Digraph\n",
    "dot = Digraph(comment='microtc pipeline', format=\"png\")\n",
    "# dot.engine = 'circo'\n",
    "dot.graph_attr['rankdir'] = 'LR'\n",
    "dot.node('i', '', style=\"invis\")\n",
    "dot.node('n', 'Normalización')\n",
    "dot.node('t', 'Tokenización')\n",
    "dot.node('w', 'Pesado')\n",
    "dot.node('c', 'Clasificación')\n",
    "dot.node('o', '', style=\"invis\")\n",
    "\n",
    "dot.edge('i', 'n', label=\"texto entrada\")\n",
    "dot.edge('n', 't', label=\"texto normalizado\")\n",
    "dot.edge('t', 'w', label=\"bolsa de palabras\")\n",
    "dot.edge('w', 'c', label=\"vector con pesos\")\n",
    "dot.edge('c', 'o', label=\"clase\")\n",
    "\n",
    "pipeline = dot.render(\"fig-pipeline\", view=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "c3871447-3727-48a1-b4ff-887ba971be91"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clasificación de texto #\n",
    "## Un enfoque basado en $\\mu TC$ ##\n",
    "\n",
    "\n",
    "**Seminario de la Sociedad Matemática Mexicana SMM 2016**\n",
    "\n",
    "\n",
    "<div>\n",
    "    Eric Sadit Téllez Avila INFOTEC \n",
    "    <estellezav@conacyt.mx> <br/>\n",
    "       CONACyT -- INFOTEC\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2a1c3de4-2d92-45aa-9a15-8102f294f125"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda #\n",
    "- ¿Qué es $\\mu TC$\n",
    "- ¿En qué consiste la tarea de clasificación de texto?\n",
    "- ¿Cómo esta compuesto $\\mu TC$?\n",
    "- Estado del arte\n",
    "- Cómo se compara $\\mu TC$ con el estado del arte\n",
    "- Qué falta en $\\mu TC$\n",
    "- Ejemplos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "02bc8966-186d-4b47-9cf1-2ad9246d0491"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorización de texto ##\n",
    "El problema consiste en, dado un texto $d$, determinar la(s) categoría(s) a la que pertenece en un conjunto $C$ de categorias, previamente conocido.\n",
    "\n",
    "Más formalmente:\n",
    "\n",
    "Dado un conjunto de categorias $\\cal{C} = \\{c_1, ..., c_m\\}$, determinar el subconjunto de categorias\n",
    "$C_d \\in \\wp(\\cal{C})$ a las que pertenece $d$. \n",
    "\n",
    "Notese que $C_t$ puede ser vacio o $\\cal{C}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "88cc9133-1951-4231-b1d3-a58428599fcd"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clasificación de texto ##\n",
    "La _clasificación_ de texto es una especialización del problema de categorización, donde $|C_d| = 1$, esto es $d$ solo puede ser asignado a una categoría.\n",
    "\n",
    "Es un problema de interés en la industria y la acádemia, con aplicaciones variadas a distintas áreas del conocimiento.\n",
    "\n",
    "- Análisis de sentimiento\n",
    "- Determinación de autoría, e.g., género, edad, estilo, etc.\n",
    "- Detección de spam\n",
    "- Categorización de noticias\n",
    "- Clasificación de idioma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "976a69bb-2f5d-4f78-9f9f-1fc016f635e5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Procesamiento de Lenguaje Natural #\n",
    "\n",
    "\n",
    "Un documento $d=s_1\\cdots s_n$ es simplemente la concatenación de símbolos $s \\in \\Sigma$. Donde, $\\Sigma$ es un _alfabeto_ de tamaño $\\sigma = |\\Sigma|$\n",
    "\n",
    "Notese qué el número de textos posibles de tamaño $n$ es $\\sigma^n$, por ejemplo, limitados a texto en inglés en Twitter se tienen\n",
    "    $$ 26^{140} \\simeq 1.248 \\times 10^{198} $$\n",
    "\n",
    "Sin emabargo, en lenguaje natural, este número no suele ser tan grande:\n",
    "  - existen reglas sobre que símbolos se pueden unir\n",
    "  - más aún, hay noción de _terminos_ o _palabras_, i.e., _morfología_\n",
    "  - también, hay reglas sobre como las palabras se pueden combinar, i.e., _sintaxis y gramática_\n",
    "\n",
    "Sin embargo, es un problema sumamente complicado, hay muchas reglas, variantes, excepciones, errores, etc.\n",
    "\n",
    "Y por si fuera poco, aunque los conceptos existen en esencia, aparecen de manera diferente en todos los lenguajes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "1c23e58d-924d-4bb0-9293-ef8f520fd240"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Además, esta el problema semántico:\n",
    "\n",
    "- un término $s_i$ tiene significados diferentes (antónimos)\n",
    "- lo contrario también existe, $s_i \\not= s_j$ pero que son idénticos en significado (sinónimos)\n",
    "- en ambos casos, el significado preciso depende del contexto\n",
    "- también hay casos _aproximados_ de todo lo anterior\n",
    "- hay muchísimos problemas abiertos\n",
    "\n",
    " **NLP** es complicado, de hecho es _AI-complete_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "76b4afb4-0abd-4f1d-a167-3d1f9abadbf5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Nuestro Enfoque #\n",
    "Por su complejidad, trabajar en NLP tiene una gran cantidad de problemas abiertos, en particular nosotros nos enfocamos en la clasificación de texto escrito de manera informal (e.g., Twitter).\n",
    "\n",
    "Para esto se utiliza un _pipeline_ estándar\n",
    "\n",
    "![Pipeline](fig-pipeline.png)\n",
    "\n",
    "No es la única opción, pero fijar el pipeline es útil como ya se expondrá.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "El enfoque teórico suele ser muy complicado, y en realidad poco efectivo en la práctica, dadas las simplificaciones necesarias para hacerlo manejable\n",
    "\n",
    "- Lógica\n",
    "- Lingüistica\n",
    "- Semántica\n",
    "\n",
    "El enfoque práctico supone muchas cosas, en particular es un tanto casuístico:\n",
    "    \n",
    "- Se fija el lenguaje\n",
    "- Se fija el problema\n",
    "- Y la raíz de todos los males, muchas veces se supone que entre más técnicas sofísticadas se usen, mejores resultados se tendrán\n",
    "\n",
    "En ambos enfoques se suele suponer que ausencia de errores de diferentes fuentes, sin embargo, es la regla cuando el texto que se analiza fue generado por usuarios de una red social, e.g. Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8a2f28db-806c-43bf-a178-38176c300102"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ¿Qué es $\\mu TC$? #\n",
    "micro TC o $\\mu TC$ es un clasificador de texto desarrollado en\n",
    "el _Laboratorio de Análisis Computacional de Grandes Cúmulos de Información_\n",
    "(o _Laboratorio de BigDATA_) de INFOTEC, sede Aguascalientes.\n",
    "\n",
    "Esta disponible para ser clonado en [https://github.com/INGEOTEC/microTC](https://github.com/INGEOTEC/microTC). Esta escrito en Python 3.5 para sacar ventaja de unicode. También se puede instalar utilizando `pip` y `conda`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6b172843-4297-41c3-a02c-0624b8371113"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En particular, nuestro enfoque se basa en _aprendizaje computacional_ y _optimización combinatoria_. Hemos probado que este esquema es muy competitivo en la práctica. Además, con la adecuada selección de las funciones podemos lograr que \n",
    "$\\mu TC$ se independiente del lenguaje y robusto a errores.\n",
    "\n",
    "Esta compuesto por:\n",
    "- una serie de funciones de transformación de texto\n",
    "- una serie de tokenizadores\n",
    "- filtros de palabras y\n",
    "- algoritmos de pesado de términos\n",
    "\n",
    "Todo esto orquestado mediante un algoritmo de optimización combinatoria\n",
    "\n",
    "Entonces, $\\mu TC$ optimiza el sub-proceso **normalización $\\rightarrow$ tokenizado** para una tarea de clasificación de texto dada. De manera más detallada, una tarea esta definida por $(D, C, f)$\n",
    "\n",
    "- $D$ es el conjunto de entrenamiento, cada elemento es un documento\n",
    "- $C$ es el conjunto de clases o etiquetas validas\n",
    " * todo $d \\in D$ tiene asociado una etiqueda $C_d \\in C$\n",
    "- $f$ es una función de aptitud $f: T \\rightarrow \\mathbb{R}^+$, i.e., una medida de que tan bien va la clasificación para un conjunto de prueba $T$\n",
    "\n",
    "El conjunto de prueba $T$ tiene la misma forma de $D$, i.e., $C_d \\in C$ para todo ${d \\in T}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "afbf9d0b-a293-4282-b281-35583ac57b2e"
    }
   },
   "source": [
    "## Lista de parametros ##\n",
    "\n",
    "### Normalizadores multilenguaje ###\n",
    "\n",
    "|   nombre  | valores             |        descripción                   |\n",
    "|-----------|---------------------|--------------------------------------|\n",
    "|\tdel-punc | yes, no | Determina si las puntuaciones deben removerse |\n",
    "|\tdel-d1   | yes, no | Determina si se deben borrar letras repetidas |\n",
    "|\tdel-diac | yes, no | Determina si los simbolos que no ocupan espacios deben ser removidos |\n",
    "|\tlc       | yes, no | Determina si los símbolos deben ser normalizados en minúsculas |\n",
    "|\temo      | remove, group, none | Controla como deben tratarse los emoticones |\n",
    "|\tnum      | remove, group, none | `........................` números |\n",
    "|\turl      | remove, group, none | `........................` urls |\n",
    "|\tusr      | remove, group, none | `........................` usuarios |\n",
    "\n",
    "\n",
    "### Normalizadores dependientes del lenguaje ###\n",
    "\n",
    "|   nombre  | valores             |        descripción                   |\n",
    "|-----------|---------------------|--------------------------------------|\n",
    "|\tstem   | yes, no | Determina si a las palabras se les aplica _stemming_. |\n",
    "|\tneg    | yes, no | Determina si los operadores de negación son manejados de manera especial |\n",
    "|\tsw | remove, group, none | Controla como los _stopwords_ son manejados |\n",
    "\n",
    "### Tokenizadores ###\n",
    "Los tokenizadores son en realidad una lista de tokenizadores, y están definidos tokenizer un elemento en $\\wp{(\\text{n-words} \\cup \\text{q-grams} \\cup \\text{skip-grams})} \\setminus \\{\\emptyset\\}$\n",
    "\n",
    "|   nombre  | valores             |        descripción                   |\n",
    "|-----------|---------------------|--------------------------------------|\n",
    "|\tn-words    | $\\{1,2,3\\}$      | Longitud de n-gramas de palabras (n-words) |\n",
    "|\tq-grams  | $\\{1,2,3,4,5,6,7\\}$ | Longitud de q-gramas de caracteres) |\n",
    "|\tskip-grams  | $\\{(2,1), (3, 1), (2, 2), (3, 2)\\}$ | skip-gram list. |\n",
    "\n",
    "### Parametros para pesado ###\n",
    "|   nombre  | valores             |        descripción                   |\n",
    "|-----------|---------------------|--------------------------------------|\n",
    "|token_min_filter | $\\{0.01, 0.03, 0.1, 0.30, -1, -5, -10\\}$ | Filtro de frequencias bajas |\n",
    "|token_max_filter | $\\{0.9, 99, 1.0\\}$ | Filtro de frequencias altas |\n",
    "|\ttfidf    | yes, no | Determina si se debe realizar un pesado TFIDF de terminos |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5702f691-7640-40a3-bd02-1c337dfb6686"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def emoformat(A, emo):\n",
    "    s = \"  \".join([a[0] for a in A if a[1] == emo])\n",
    "    return s[:1000] + \"...\"\n",
    "    \n",
    "with open('microTC/microtc/resources/emoticons.json') as f:\n",
    "    A = list(json.load(f).items())\n",
    "    A.sort()\n",
    "    S = dict(\n",
    "        pos=emoformat(A, '_pos'),\n",
    "        neg=emoformat(A, '_neg'),\n",
    "        neu=emoformat(A, '_neu'),\n",
    "        none=emoformat(A, '_none'),\n",
    "    )\n",
    "\n",
    "output = [\"## Emoticones y emojis clasificados por sentimiento ##\"]\n",
    "for k, v in S.items():\n",
    "    output.append(\"## Clase `{0}` ##\".format(k.upper()))\n",
    "    output.append(v)\n",
    "\n",
    "Markdown(\"\\n\".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b9e49694-b1e2-47b7-a6ee-c118b8dccf41"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "text = \"anita lava la tina\"\n",
    "tokenizers = [1, 2, 3, -1, -2, (2,1), (2,2)]\n",
    "\n",
    "num = 1\n",
    "output = []\n",
    "for ltokens in range(len(tokenizers)):\n",
    "    output.append('## Combinaciones de tamaño {0} ##'.format(ltokens+1))\n",
    "    output.append('|id|combinación|tokens|')\n",
    "    output.append('|--|-----------|------|')\n",
    "    for comb in combinations(tokenizers, ltokens+1):\n",
    "        model = TextModel([], token_list=comb)\n",
    "        output.append(\"|{0}|{1}|{2}|\".format(num, comb, \", \".join(model.tokenize(text))))\n",
    "        num += 1\n",
    "\n",
    "Markdown(\"\\n\".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3f510c80-93c5-4e42-a5bf-3b0b24fd59ca"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sobre el pesado ##\n",
    "El pesado de tokens esta fijo a TFIDF. Su nombre viene de la formulación $tf \\times idf$\n",
    "\n",
    "$tf$ es _term frequency_; es una medida de importancia **local** del término $t$ en el documento $d$, de manera normalizada esta definida como:\n",
    "    $$tf(t,d) = \\frac{freq(t, d)}{\\max_{w \\in d}{freq(w, d)}}$$\n",
    "entre más veces aparece en el documento $d$, $t$ es más importante\n",
    "\n",
    "$idf$ quiere decir _inverse document frequency_; es una medida **global** a la colección $D$, esta definida como:\n",
    "$$ idf(t,d) = log{\\frac{|D|}{1+|{d \\in D: t \\in d}|}} $$\n",
    "entre más veces aparece $t$ en la colección, el término es más común y menos discriminante; por lo tanto, menos importante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0cedcc81-a913-4453-81aa-3386a4bbc972"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sobre el clasificador ##\n",
    "El clasificador es un algoritmo de aprendizaje computacional que dado un objeto, decide finalmente la etiqueta o clase de ese objeto. Tiene dos etapas bien definidas\n",
    "\n",
    "- **Entrenamiento.** Dado un conjunto de ejemplos en un espacio vectorial, con etiquetas, el algoritmo intenta _aprender_ las características que definen cada clase\n",
    "- **Predicción.** La idea es que una vez entrenado, el algoritmo puede recibir objetos no vistos durante la etapa de entrenamiento y asignales la clase adecuada\n",
    "\n",
    "En particular, esta fijo como un _Support Vector Machine_ (SVM) con kernel lineal\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "b0433df0-cf31-4f76-8bbe-2fbdab949ff3",
    "theme": {
     "b0433df0-cf31-4f76-8bbe-2fbdab949ff3": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "b0433df0-cf31-4f76-8bbe-2fbdab949ff3",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         240,
         241,
         235
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         56,
         61,
         61
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         81,
         72,
         61
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         0,
         0,
         0
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "EB Garamond",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "EB Garamond",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "EB Garamond",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "EB Garamond",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "EB Garamond"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "EB Garamond"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "EB Garamond"
       },
       "li": {
        "color": "mainColor",
        "font-family": "EB Garamond",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "EB Garamond",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "EB Garamond",
       "font-size": 5
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
